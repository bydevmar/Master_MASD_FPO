{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd3815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des bibliothèques pour la manipulation de données\n",
    "import pandas as pd  # Pandas pour la manipulation de données tabulaires\n",
    "\n",
    "# Import des bibliothèques pour la visualisation de données\n",
    "import matplotlib.pyplot as plt  # Matplotlib pour la création de graphiques\n",
    "import seaborn as sns  # Seaborn pour des visualisations statistiques attrayantes\n",
    "\n",
    "# Import des bibliothèques de traitement du langage naturel (NLP)\n",
    "import nltk  # Bibliothèque NLTK pour le traitement du langage naturel\n",
    "from nltk.corpus import stopwords  # Liste de mots vides pour le prétraitement\n",
    "from nltk.tokenize import word_tokenize  # Tokenisation de mots\n",
    "import string  # Opérations de chaînes de caractères\n",
    "\n",
    "# Téléchargement des ressources nécessaires de NLTK (commenté car téléchargement uniquement nécessaire une fois)\n",
    "# nltk.download('stopwords')  # Téléchargement des mots vides\n",
    "# nltk.download('punkt')  # Téléchargement des modèles de tokenisation\n",
    "\n",
    "# Import des composants de la bibliothèque Transformers pour l'utilisation de modèles pré-entraînés\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification  # Transformers pour NLP\n",
    "\n",
    "# Import de la fonction softmax pour normaliser les scores de classification\n",
    "from scipy.special import softmax  # SciPy pour des opérations mathématiques avancées\n",
    "\n",
    "# Import de la barre de progression pour les boucles (utile pour surveiller l'avancement)\n",
    "from tqdm.notebook import tqdm  # TQDM pour une barre de progression interactive\n",
    "\n",
    "import re  # Module d'expressions régulières en Python pour la manipulation de texte\n",
    "\n",
    "import numpy as np  # NumPy pour des opérations numériques avancées\n",
    "\n",
    "from wordcloud import WordCloud  # WordCloud pour la création de nuages de mots\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662864fb",
   "metadata": {},
   "source": [
    "## Anaconda et Jupyter : distinction et usages\n",
    "\n",
    "**Anaconda** et **Jupyter** sont deux outils importants utilisés dans le domaine de la science des données et du machine learning, mais ils jouent des rôles distincts :\n",
    "\n",
    "**1. Anaconda**\n",
    "\n",
    "* **Définition :** Anaconda est une distribution logicielle gratuite et open-source qui regroupe un ensemble de paquets et d'outils préinstallés pour le data science et le machine learning. \n",
    "* **Fonctionnalités principales :**\n",
    "    * **Gestionnaire de paquets conda :** Installation, mise à jour et suppression de paquets Python et R.\n",
    "    * **Environnements virtuels:** Création d'environnements isolés pour gérer les dépendances de bibliothèques pour différents projets.\n",
    "    * **Navigateur Anaconda (interface graphique optionnelle) :** Interface conviviale pour la gestion des paquets, des environnements et des applications liées à la data science.\n",
    "    * **Inclusion de Python et de bibliothèques populaires :** Python (langage de programmation central), NumPy, Pandas, Matplotlib, Scikit-learn, etc.\n",
    "\n",
    "**En résumé, Anaconda est une plateforme complète qui fournit les outils nécessaires pour démarrer rapidement des projets de data science et de machine learning.**\n",
    "\n",
    "**2. Jupyter**\n",
    "\n",
    "* **Définition :** Jupyter est une application web open-source utilisée pour la création de notebooks interactifs. Ces notebooks permettent de combiner du code, du texte enrichi, des visualisations et des résultats d'exécution dans un seul document.\n",
    "* **Fonctionnalités principales :**\n",
    "    * **Création de notebooks interactifs :** Écriture de code (Python, R, Julia, etc.), ajout de texte explicatif, génération de visualisations et observation des résultats en direct.\n",
    "    * **Partage et collaboration :** Les notebooks Jupyter peuvent être facilement partagés et modifiés par des collaborateurs.\n",
    "    * **Support de différents langages de programmation :** Jupyter n'est pas spécifique à Python et peut être utilisé avec divers langages.\n",
    "\n",
    "**En résumé, Jupyter est un outil interactif permettant de développer du code, documenter son travail et visualiser les résultats de manière interactive, facilitant l'apprentissage, l'analyse et la communication des résultats.**\n",
    "\n",
    "**Relation entre Anaconda et Jupyter**\n",
    "\n",
    "Anaconda et Jupyter sont souvent utilisés de concert. Anaconda installe Jupyter Notebook par défaut, ce qui permet d'utiliser facilement les bibliothèques et les environnements gérés par Anaconda dans vos notebooks interactifs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e003da9",
   "metadata": {},
   "source": [
    "## Définition et utilisation de chaque bibliothèque importée :\n",
    "\n",
    "**1. Pandas:**\n",
    "\n",
    "* **Définition:** Bibliothèque open-source pour la manipulation de données tabulaires.\n",
    "* **Utilisation:**\n",
    "    * Lire et écrire des données dans différents formats (CSV, Excel, etc.).\n",
    "    * Nettoyer et transformer les données.\n",
    "    * Analyser les données et calculer des statistiques.\n",
    "    * Visualiser les données.\n",
    "\n",
    "**2. Matplotlib:**\n",
    "\n",
    "* **Définition:** Bibliothèque open-source pour la création de graphiques et de visualisations scientifiques.\n",
    "* **Utilisation:**\n",
    "    * Créer différents types de graphiques (barres, courbes, diagrammes, etc.).\n",
    "    * Personnaliser les graphiques avec des couleurs, des titres, des légendes, etc.\n",
    "    * Intégrer des graphiques dans des notebooks ou des applications web.\n",
    "\n",
    "**3. Seaborn:**\n",
    "\n",
    "* **Définition:** Bibliothèque open-source basée sur Matplotlib pour créer des visualisations statistiques attrayantes.\n",
    "* **Utilisation:**\n",
    "    * Créer des graphiques à barres, des courbes de distribution, des heatmaps, etc.\n",
    "    * Appliquer des thèmes et des palettes de couleurs prédéfinies.\n",
    "    * Faciliter la création de visualisations statistiques complexes.\n",
    "\n",
    "**4. NLTK:**\n",
    "\n",
    "* **Définition:** Bibliothèque open-source pour le traitement du langage naturel (NLP).\n",
    "* **Utilisation:**\n",
    "    * Tokeniser les mots d'un texte.\n",
    "    * Supprimer les mots vides.\n",
    "    * Lemmatiser les mots.\n",
    "    * Taguer les parties du discours.\n",
    "    * Analyser le sentiment d'un texte.\n",
    "\n",
    "**5. Transformers:**\n",
    "\n",
    "* **Définition:** Bibliothèque open-source pour l'utilisation de modèles de langage pré-entraînés pour le NLP.\n",
    "* **Utilisation:**\n",
    "    * Charger un modèle de langage pré-entraîné.\n",
    "    * Encodage de texte en vecteurs.\n",
    "    * Classification de texte.\n",
    "    * Génération de texte.\n",
    "\n",
    "**6. SciPy:**\n",
    "\n",
    "* **Définition:** Bibliothèque open-source pour des opérations mathématiques avancées.\n",
    "* **Utilisation:**\n",
    "    * Calculer des statistiques avancées.\n",
    "    * Résoudre des équations différentielles.\n",
    "    * Optimiser des fonctions.\n",
    "\n",
    "**7. tqdm:**\n",
    "\n",
    "* **Définition:** Bibliothèque open-source pour afficher une barre de progression interactive dans les boucles.\n",
    "* **Utilisation:**\n",
    "    * Surveiller l'avancement d'un processus long.\n",
    "    * Afficher des informations sur l'itération en cours.\n",
    "\n",
    "**8. re:**\n",
    "\n",
    "* **Définition:** Module Python pour la manipulation de texte avec des expressions régulières.\n",
    "* **Utilisation:**\n",
    "    * Rechercher et remplacer des motifs dans le texte.\n",
    "    * Extraire des informations du texte.\n",
    "    * Valider des formats de texte.\n",
    "\n",
    "**9. NumPy:**\n",
    "\n",
    "* **Définition:** Bibliothèque open-source pour des opérations numériques avancées sur des tableaux multidimensionnels.\n",
    "* **Utilisation:**\n",
    "    * Effectuer des calculs mathématiques sur des tableaux.\n",
    "    * Créer des matrices et des vecteurs.\n",
    "    * Appliquer des fonctions mathématiques aux tableaux.\n",
    "\n",
    "**10. WordCloud:**\n",
    "\n",
    "* **Définition:** Bibliothèque open-source pour créer des nuages de mots à partir de texte.\n",
    "* **Utilisation:**\n",
    "    * Générer des nuages de mots à partir d'un corpus de texte.\n",
    "    * Personnaliser les nuages de mots avec des couleurs, des polices, etc.\n",
    "\n",
    "**11. BeautifulSoup:**\n",
    "\n",
    "* **Définition:** Bibliothèque open-source pour analyser et extraire des données de documents HTML et XML.\n",
    "* **Utilisation:**\n",
    "    * Extraire du texte et des attributs des balises HTML.\n",
    "    * Naviguer dans l'arborescence du document.\n",
    "    * Analyser la structure du document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f9fa47",
   "metadata": {},
   "source": [
    "La moyenne (mean) et la médiane (median) sont deux mesures statistiques utilisées pour représenter le centre d'une distribution de données. Voici leurs différences principales :\n",
    "\n",
    "1. **Moyenne (mean) :**\n",
    "   - La moyenne est la somme de toutes les valeurs divisée par le nombre total de valeurs.\n",
    "   - Elle est sensible aux valeurs extrêmes, également appelées valeurs aberrantes (outliers). Cela signifie qu'une seule valeur extrême peut considérablement affecter la moyenne.\n",
    "   - La moyenne est souvent utilisée lorsque les données sont relativement symétriques et sans valeurs aberrantes significatives.\n",
    "\n",
    "2. **Médiane (median) :**\n",
    "   - La médiane est la valeur au milieu de l'ensemble de données lorsqu'il est trié par ordre croissant ou décroissant.\n",
    "   - Contrairement à la moyenne, la médiane n'est pas influencée par les valeurs extrêmes. Elle ne considère que la valeur centrale, indépendamment de la magnitude des autres valeurs.\n",
    "   - La médiane est utile lorsque les données sont asymétriques ou contiennent des valeurs aberrantes.\n",
    "\n",
    "En résumé, la moyenne est sensible aux valeurs extrêmes, tandis que la médiane ne l'est pas. Le choix entre la moyenne et la médiane dépend de la nature des données et de l'objectif de l'analyse statistique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c939fb54",
   "metadata": {},
   "source": [
    "## stop words\n",
    "Les \"stop words\", littéralement \"mots vides\" en français, sont des mots très courants et généralement peu informatifs qui sont souvent supprimés lors du traitement de texte ou de la recherche d'informations textuelles. Ces mots sont généralement des articles, des prépositions, des conjonctions et d'autres mots très fréquents dans une langue donnée.\n",
    "\n",
    "Les \"stop words\" sont exclus des analyses textuelles telles que l'indexation de documents ou la recherche d'informations, car ils n'apportent pas de sens significatif à la compréhension du contenu. En les retirant, on peut réduire la taille du corpus de texte et améliorer la précision des analyses textuelles en se concentrant sur les mots qui portent une signification plus distinctive.\n",
    "\n",
    "Exemples courants de \"stop words\" en anglais incluent : \"the\", \"is\", \"and\", \"of\", \"in\", \"to\", \"a\", etc. Pour chaque langue, il existe une liste spécifique de \"stop words\" adaptée à cette langue, basée sur la fréquence d'utilisation des mots dans des textes courants et leur faible contribution à la sémantique générale du texte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60de1376",
   "metadata": {},
   "source": [
    "## Pre-traitement de text :\n",
    "Ce code effectue un prétraitement du texte en supprimant les balises HTML, les caractères non alphabétiques, en convertissant le texte en minuscules et en supprimant les \"stop words\" (mots vides). Voici une explication ligne par ligne :\n",
    "\n",
    "1. `stop_words = set(stopwords.words('english'))`: Cette ligne charge un ensemble de \"stop words\" en anglais à partir de la bibliothèque NLTK (Natural Language Toolkit). Les \"stop words\" sont stockés dans un ensemble pour une recherche efficace.\n",
    "\n",
    "2. `def preprocess_text(text):`: Cette ligne définit une fonction nommée `preprocess_text` qui prend en entrée un texte à prétraiter.\n",
    "\n",
    "3. `if re.match(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text):`: Cette condition vérifie si le texte ressemble à une URL à l'aide d'une expression régulière. Si c'est le cas, le contenu de l'URL pourrait être récupéré, mais dans ce code, il est laissé en commentaire (dans le `pass`).\n",
    "\n",
    "4. Sinon, le texte est considéré comme un texte brut et les balises HTML sont supprimées à l'aide de BeautifulSoup.\n",
    "\n",
    "5. `text = re.sub(r'[^a-zA-Z\\s]', '', text)`: Cette ligne supprime tous les caractères non alphabétiques du texte.\n",
    "\n",
    "6. `text = text.lower()`: Cette ligne convertit tout le texte en minuscules pour normaliser le texte.\n",
    "\n",
    "7. `words = word_tokenize(text)`: Cette ligne divise le texte en mots individuels (tokenization) à l'aide de la fonction `word_tokenize` de NLTK.\n",
    "\n",
    "8. `words = [word for word in words if word not in stop_words]`: Cette ligne filtre les \"stop words\" du texte en supprimant les mots qui sont présents dans l'ensemble des \"stop words\" défini précédemment.\n",
    "\n",
    "9. `return ' '.join(words)`: Cette ligne réassemble les mots traités en une seule chaîne de caractères, en les séparant par des espaces.\n",
    "\n",
    "10. `tqdm.pandas()`: Cette ligne active l'utilisation de la fonction `progress_apply()` de pandas avec tqdm pour afficher une barre de progression lors de l'application de la fonction `preprocess_text` à chaque ligne du DataFrame.\n",
    "\n",
    "11. `data['Processed_Text'] = data['Text'].progress_apply(preprocess_text)`: Cette ligne applique la fonction `preprocess_text` à chaque texte de la colonne 'Text' du DataFrame `data` et stocke le résultat dans une nouvelle colonne appelée 'Processed_Text'.\n",
    "\n",
    "12. `data.head()`: Cette ligne affiche les premières lignes du DataFrame après le prétraitement du texte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5975b749",
   "metadata": {},
   "source": [
    "## Models \n",
    "Bien sûr, voici une liste complète des modèles et bibliothèques populaires pour l'analyse des sentiments en Python :\n",
    "\n",
    "1. **TextBlob** :\n",
    "   - TextBlob est une bibliothèque Python simple et facile à utiliser pour le traitement du langage naturel, qui offre des fonctionnalités d'analyse des sentiments.\n",
    "   - Il fournit une méthode `sentiment` qui renvoie une polarité (positif, négatif ou neutre) et une subjectivité du texte.\n",
    "\n",
    "2. **VADER (Valence Aware Dictionary and sEntiment Reasoner)** :\n",
    "   - VADER est un lexique et un outil de traitement des sentiments spécialement conçu pour les médias sociaux.\n",
    "   - Il est intégré dans la bibliothèque NLTK (Natural Language Toolkit) et fournit une analyse des sentiments basée sur des règles et des scores lexicaux.\n",
    "\n",
    "3. **TextBlob avec Pattern Analyzer** :\n",
    "   - TextBlob dispose également d'un analyseur de modèle basé sur la bibliothèque Pattern.\n",
    "   - Cet analyseur fournit une analyse des sentiments basée sur un modèle de classification de phrases entraîné sur des données annotées.\n",
    "\n",
    "4. **scikit-learn** :\n",
    "   - La bibliothèque scikit-learn peut être utilisée pour l'analyse des sentiments en utilisant des techniques de classification supervisée.\n",
    "   - Vous pouvez entraîner des modèles de classification tels que Naive Bayes, SVM (Support Vector Machines) ou des méthodes basées sur les arbres de décision sur des données annotées avec des sentiments pour prédire les sentiments sur de nouveaux textes.\n",
    "\n",
    "5. **Transformers (Hugging Face)** :\n",
    "   - Les modèles basés sur les Transformers, tels que BERT, RoBERTa, DistilBERT, etc., peuvent également être utilisés pour l'analyse des sentiments en fine-tunant ces modèles sur des tâches d'analyse de sentiments.\n",
    "   - La bibliothèque Hugging Face fournit une API simple et un accès à de nombreux modèles pré-entraînés pour le traitement du langage naturel, y compris des modèles pour l'analyse des sentiments.\n",
    "\n",
    "6. **Transformers (Hugging Face) avec RoBERTa** :\n",
    "   - RoBERTa est un modèle de langage basé sur les Transformers, entraîné par Facebook.\n",
    "   - Vous pouvez utiliser RoBERTa pour l'analyse des sentiments en fine-tunant le modèle sur des tâches d'analyse de sentiments.\n",
    "   - La bibliothèque Hugging Face fournit une interface simple pour charger et utiliser des modèles RoBERTa pré-entraînés pour l'analyse des sentiments.\n",
    "   - Avec RoBERTa, vous pouvez obtenir des résultats précis sur une variété de tâches liées à l'analyse des sentiments, y compris la classification binaire (positif/négatif) ou la classification multi-classes (positif/neutre/négatif).\n",
    "\n",
    "Ces outils et bibliothèques offrent différentes approches pour l'analyse des sentiments, chacune avec ses propres avantages et inconvénients en fonction du contexte d'application et des besoins spécifiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb6a04b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m MODEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcardiffnlp/twitter-roberta-base-sentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Initialisation du tokenizer avec le modèle pré-entraîné\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Initialisation du modèle de classification de séquence basé sur RoBERTa pour Twitter\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Définition du modèle pré-entraîné à utiliser (dans ce cas, le modèle de sentiment basé sur RoBERTa pour Twitter)\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "# Initialisation du tokenizer avec le modèle pré-entraîné\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# Initialisation du modèle de classification de séquence basé sur RoBERTa pour Twitter\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce560e84",
   "metadata": {},
   "source": [
    "## Init du MODEL \n",
    "Ce code initialise un modèle pré-entraîné pour l'analyse des sentiments basé sur RoBERTa pour Twitter en utilisant la bibliothèque Transformers de Hugging Face. Voici une explication ligne par ligne :\n",
    "\n",
    "1. `MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\"` :\n",
    "   - Cette ligne définit le nom du modèle pré-entraîné à utiliser. Dans ce cas, le modèle choisi est \"twitter-roberta-base-sentiment\", qui est spécialement entraîné pour l'analyse des sentiments sur des tweets.\n",
    "\n",
    "2. `tokenizer = AutoTokenizer.from_pretrained(MODEL)` :\n",
    "   - Cette ligne initialise le tokenizer associé au modèle pré-entraîné spécifié. Le tokenizer est responsable de la tokenisation du texte en entrée, c'est-à-dire de la division du texte en tokens compréhensibles par le modèle. \n",
    "   - `AutoTokenizer.from_pretrained(MODEL)` charge automatiquement le tokenizer approprié pour le modèle spécifié.\n",
    "\n",
    "3. `model = AutoModelForSequenceClassification.from_pretrained(MODEL)` :\n",
    "   - Cette ligne initialise le modèle de classification de séquence basé sur RoBERTa pour Twitter.\n",
    "   - `AutoModelForSequenceClassification.from_pretrained(MODEL)` charge automatiquement le modèle pré-entraîné spécifié et configure le modèle pour effectuer la classification de séquence, où l'objectif est de prédire la classe (ou le label) d'une séquence donnée, dans ce cas, le sentiment du tweet.\n",
    "\n",
    "En résumé, ce code charge un modèle pré-entraîné et son tokenizer associé spécifiquement conçus pour l'analyse des sentiments sur des tweets en utilisant RoBERTa. Ces modèles pré-entraînés sont utiles pour effectuer rapidement et efficacement l'analyse des sentiments sur de grands volumes de données de texte provenant de Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cc941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_scores(text):\n",
    "    encoded_text = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_text)\n",
    "    scores = output.logits.detach().numpy()\n",
    "    scores = softmax(scores, axis=1)\n",
    "    scores_dict = {\n",
    "        'rta_neg': scores[0, 0],\n",
    "        'rta_neu': scores[0, 1],\n",
    "        'rta_pos': scores[0, 2]\n",
    "    }\n",
    "    return scores_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108329b4",
   "metadata": {},
   "source": [
    "Ce code définit une fonction `roberta_scores(text)` qui prend en entrée un texte et renvoie un dictionnaire contenant les scores de sentiment prédits pour ce texte par le modèle RoBERTa. Voici une explication ligne par ligne :\n",
    "\n",
    "1. `encoded_text = tokenizer(text, return_tensors='pt')` :\n",
    "   - Cette ligne utilise le tokenizer pour encoder le texte en une représentation compréhensible par le modèle RoBERTa.\n",
    "   - Le texte est converti en tokens et encodé sous forme de tenseurs PyTorch (`'pt'`).\n",
    "   - Les tokens encodés sont stockés dans la variable `encoded_text`.\n",
    "\n",
    "2. `output = model(**encoded_text)` :\n",
    "   - Cette ligne passe les tokens encodés au modèle RoBERTa pour obtenir les prédictions de sentiment.\n",
    "   - Le modèle est appelé avec les tokens encodés en utilisant `**encoded_text` pour transmettre les arguments nommés.\n",
    "   - Les prédictions sont stockées dans la variable `output`.\n",
    "\n",
    "3. `scores = output.logits.detach().numpy()` :\n",
    "   - Cette ligne extrait les scores de sortie prédits par le modèle à partir de l'objet `output`.\n",
    "   - Les scores sont généralement des logits, c'est-à-dire des valeurs non normalisées, et sont détachés du graphe de calcul et convertis en un tableau NumPy pour un traitement ultérieur.\n",
    "   - Les scores sont stockés dans la variable `scores`.\n",
    "\n",
    "4. `scores = softmax(scores, axis=1)` :\n",
    "   - Cette ligne applique la fonction softmax aux scores pour normaliser les valeurs et obtenir des probabilités de classe.\n",
    "   - La fonction softmax est appliquée le long de l'axe des colonnes (axis=1) pour obtenir des probabilités pour chaque classe de sentiment.\n",
    "   - Les scores normalisés sont stockés dans la variable `scores`.\n",
    "\n",
    "5. `scores_dict = { 'rta_neg': scores[0, 0], 'rta_neu': scores[0, 1], 'rta_pos': scores[0, 2] }` :\n",
    "   - Cette ligne crée un dictionnaire contenant les scores de sentiment prédits.\n",
    "   - Les scores sont renommés en utilisant des clés descriptives ('rta_neg' pour négatif, 'rta_neu' pour neutre et 'rta_pos' pour positif) et sont associés à leurs valeurs correspondantes dans le tableau de scores normalisés.\n",
    "\n",
    "6. `return scores_dict` :\n",
    "   - Cette ligne renvoie le dictionnaire contenant les scores de sentiment prédits pour le texte donné par le modèle RoBERTa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9280e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "# Diviser les données en 10 parties\n",
    "num_parts = 10\n",
    "part_size = len(data) // num_parts\n",
    "for part in range(num_parts):\n",
    "    start_idx = part * part_size\n",
    "    end_idx = min((part + 1) * part_size, len(data))\n",
    "    part_data = data.iloc[start_idx:end_idx]\n",
    "\n",
    "    for i, row in tqdm(part_data.iterrows(), total=len(part_data)):\n",
    "        try:\n",
    "            text = row['Processed_Text']  \n",
    "            my_id = row['Id']\n",
    "            # Effectuer une analyse de sentiment avec RoBERTa\n",
    "            roberta_result = roberta_scores(text)\n",
    "            # Conserver uniquement les résultats de RoBERTa\n",
    "            res[my_id] = roberta_result\n",
    "        except Exception as e:\n",
    "            print(f'Problème dans ID {my_id} : {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92ab818",
   "metadata": {},
   "source": [
    "Ce code effectue une analyse de sentiment sur les données textuelles en utilisant le modèle RoBERTa, puis stocke les résultats dans un dictionnaire pour chaque entrée de données. Voici une explication ligne par ligne :\n",
    "\n",
    "1. `res = {}` : Cette ligne initialise un dictionnaire vide `res` qui sera utilisé pour stocker les résultats de l'analyse de sentiment.\n",
    "\n",
    "2. `num_parts = 10` : Le nombre de parties dans lesquelles les données seront divisées. Cela peut être ajusté en fonction de la taille des données et des ressources disponibles.\n",
    "\n",
    "3. `part_size = len(data) // num_parts` : Cette ligne calcule la taille de chaque partie des données en divisant le nombre total d'entrées de données par le nombre de parties.\n",
    "\n",
    "4. La boucle `for part in range(num_parts):` itère sur chaque partie des données.\n",
    "\n",
    "5. `start_idx = part * part_size` et `end_idx = min((part + 1) * part_size, len(data))` : Ces lignes calculent les indices de début et de fin pour chaque partie des données en fonction de la taille de la partie actuelle.\n",
    "\n",
    "6. `part_data = data.iloc[start_idx:end_idx]` : Cette ligne extrait la partie correspondante des données en utilisant les indices calculés.\n",
    "\n",
    "7. La boucle `for i, row in tqdm(part_data.iterrows(), total=len(part_data)):` itère sur chaque ligne de la partie de données, en affichant une barre de progression avec `tqdm`.\n",
    "\n",
    "8. `text = row['Processed_Text']` et `my_id = row['Id']` : Ces lignes extraient le texte prétraité et l'identifiant de chaque ligne de données.\n",
    "\n",
    "9. `roberta_result = roberta_scores(text)` : Cette ligne appelle une fonction `roberta_scores()` (non fournie dans ce code) pour effectuer une analyse de sentiment sur le texte en utilisant le modèle RoBERTa. Les résultats sont stockés dans la variable `roberta_result`.\n",
    "\n",
    "10. `res[my_id] = roberta_result` : Cette ligne stocke les résultats de l'analyse de sentiment dans le dictionnaire `res`, en utilisant l'identifiant comme clé pour chaque entrée de données.\n",
    "\n",
    "11. `except Exception as e:` : Cette clause `try-except` capture toute exception qui pourrait survenir lors de l'analyse de sentiment. Si une exception est levée, elle est affichée avec un message d'erreur spécifiant l'ID de l'entrée de données concernée.\n",
    "\n",
    "En résumé, ce code divise les données en parties, puis effectue une analyse de sentiment sur chaque partie en utilisant le modèle RoBERTa. Les résultats sont stockés dans un dictionnaire, associant chaque identifiant de données aux scores de sentiment prédits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
